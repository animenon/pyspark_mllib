from pyspark.ml.feature import MinMaxScaler
from pyspark import SparkContext
from pyspark.sql import SQLContext

sc = SparkContext("local", "samp")
sqlContext = SQLContext(sc)
data = sqlContext.read.format("libsvm").load("D:\Spark\spark-1.6.1-bin-hadoop2.6\data\mllib\sample_libsvm_data.txt")
indexer = MinMaxScaler(inputCol="features", outputCol="scaledFeatures")
indexerData = indexer.fit(data)
indexedData = indexerData.transform(data)
indexedData.show()
"""OUTPUT

+-----+--------------------+--------------------+
|label|            features|      scaledFeatures|
+-----+--------------------+--------------------+
|  0.0|(692,[127,128,129...|[0.5,0.5,0.5,0.5,...|
|  1.0|(692,[158,159,160...|[0.5,0.5,0.5,0.5,...|
|  1.0|(692,[124,125,126...|[0.5,0.5,0.5,0.5,...|
|  1.0|(692,[152,153,154...|[0.5,0.5,0.5,0.5,...|
|  1.0|(692,[151,152,153...|[0.5,0.5,0.5,0.5,...|
|  0.0|(692,[129,130,131...|[0.5,0.5,0.5,0.5,...|
|  1.0|(692,[158,159,160...|[0.5,0.5,0.5,0.5,...|
|  1.0|(692,[99,100,101,...|[0.5,0.5,0.5,0.5,...|
|  0.0|(692,[154,155,156...|[0.5,0.5,0.5,0.5,...|
|  0.0|(692,[127,128,129...|[0.5,0.5,0.5,0.5,...|
|  1.0|(692,[154,155,156...|[0.5,0.5,0.5,0.5,...|
|  0.0|(692,[153,154,155...|[0.5,0.5,0.5,0.5,...|
|  0.0|(692,[151,152,153...|[0.5,0.5,0.5,0.5,...|
|  1.0|(692,[129,130,131...|[0.5,0.5,0.5,0.5,...|
|  0.0|(692,[154,155,156...|[0.5,0.5,0.5,0.5,...|
|  1.0|(692,[150,151,152...|[0.5,0.5,0.5,0.5,...|
|  0.0|(692,[124,125,126...|[0.5,0.5,0.5,0.5,...|
|  0.0|(692,[152,153,154...|[0.5,0.5,0.5,0.5,...|
|  1.0|(692,[97,98,99,12...|[0.5,0.5,0.5,0.5,...|
|  1.0|(692,[124,125,126...|[0.5,0.5,0.5,0.5,...|
+-----+--------------------+--------------------+
only showing top 20 rows
"""